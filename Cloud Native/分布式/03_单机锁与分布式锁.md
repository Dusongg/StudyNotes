#  Golang单机锁

## 锁升级

`sync.Mutex`先“乐观”后“悲观”`

> 乐观锁的CAS实现:
>
> ```go
> func (m *Mutex) Lock() {
>     if atomic.CompareAndSwapInt32(&m.state, 0, mutexLocked) {
>         return
>     }
>     // Slow path (outlined so that the fast path can be inlined)
>     m.lockSlow()
> }
> ```
>
> 

自旋模式转为阻塞模式的具体条件如下：

- 自旋累计达到 4 次仍未取得战果；
- CPU 单核或仅有单个 P 调度器；（此时自旋，其他 goroutine 根本没机会释放锁，自旋纯属空转）；
- 当前 P 的执行队列中仍有待执行的 G. （避免因自旋影响到 GMP 调度效率）.



## 饥饿模式

1. 正常模式

在这种模式下，当前 Goroutine 可以直接尝试获取锁（即便队列中有等待 Goroutine）。如果加锁失败，当前 Goroutine 会加入等待队列。

2. 饥饿模式

- **什么时候切换**：当阻塞队列存在 goroutine 等锁超过 1ms 而不得，则进入饥饿模式；（当阻塞队列已清空，或取得锁的 goroutine 等锁时间已低于 1ms 时，则回到正常模式.）

- **饥饿模式下**，锁的所有权按照阻塞队列的顺序<u>FIFO</u>进行依次传递. 新 goroutine 进行流程时不得抢锁，而是进入队列尾部排队.



## 核心数据结构

```go
	type Mutex struct {
    state int32
    sema  uint32  //用于阻塞和唤醒 goroutine 的信号量.
}
```



Mutex.state:

![image-20250113下午14620494](https://typora-dusong.oss-cn-chengdu.aliyuncs.com/image-20250113%E4%B8%8B%E5%8D%8814620494.png)

# 分布式锁

<img src="https://typora-dusong.oss-cn-chengdu.aliyuncs.com/image-20250113%E4%B8%8B%E5%8D%8830001841.png" alt="image-20250113下午30001841" style="zoom:50%;" />

- **主动轮询型**：该模型类似于单机锁中的主动轮询 + cas 乐观锁模型，取锁方会持续对分布式锁发出尝试获取动作，如果锁已被占用则会不断发起重试，直到取锁成功为止
-  **watch 回调型**：在取锁方发现锁已被他人占用时，会创建 watcher 监视器订阅锁的释放事件，随后不再发起主动取锁的尝试；当锁被释放后，取锁方能通过之前创建的 watcher 感知到这一变化，然后再重新发起取锁的尝试动作

## 1️⃣[Redis实现](../../Backend/Redis/Redis.md/#7.4 Redis分布式锁)

- **如何避免死锁问题**

  设置过期时间（`SETEX`）——〉务处理流程中的耗时超过了设置的过期时间阈值，锁提前释放 ——〉**redission看门狗策略**

- **弱一致性问题**：

  - 前提：为避免单点故障问题，redis 会基于主从复制的方式实现数据备份. （以哨兵机制为例，哨兵会持续监听 master 节点的健康状况，倘若 master 节点发生故障，哨兵会负责扶持 slave 节点上位，以保证整个集群能够正常对外提供服务）. 此外，在 CAP 体系中，redis 走的是 **AP** 路线，为保证服务的吞吐性能，<u>主从节点之间的数据同步是异步延迟进行的</u>.

  - 问题：在master节点上加锁成功之后，在同步到slave节点之前，master宕机，导致多个线程占有锁
  - 解决方案：`redis 红锁`

### 看门狗(watch dog)策略

<img src="https://typora-dusong.oss-cn-chengdu.aliyuncs.com/image-20250114%E4%B8%8B%E5%8D%8814722951.png" alt="image-20250114下午14722951" style="zoom:50%;" />

- **加锁时启动看门狗**

```go
// Lock 加锁.
func (r *RedisLock) Lock(ctx context.Context) (err error) {
    defer func() {
        if err != nil {
            return
        }
        // 加锁成功的情况下，会启动看门狗
        // 关于该锁本身是不可重入的，所以不会出现同一把锁下看门狗重复启动的情况
        r.watchDog(ctx)
    }()
    // ...
}
```

```go
// 启动看门狗
func (r *RedisLock) watchDog(ctx context.Context) {
    // 1. 非看门狗模式，不处理
    if !r.watchDogMode {
        return
    }


    // 2. 确保之前启动的看门狗已经正常回收
    for !atomic.CompareAndSwapInt32(&r.runningDog, 0, 1) {
    }


    // 3. 启动看门狗
    ctx, r.stopDog = context.WithCancel(ctx)
    go func() {
        defer func() {
            atomic.StoreInt32(&r.runningDog, 0)
        }()
        r.runWatchDog(ctx)
    }()
}
```

```go
func (r *RedisLock) runWatchDog(ctx context.Context) {
    ticker := time.NewTicker(WatchDogWorkStepSeconds * time.Second)
    defer ticker.Stop()


    for range ticker.C {
        select {
        case <-ctx.Done():
            return
        default:
        }


        // 看门狗负责在用户未显式解锁时，持续为分布式锁进行续期
        // 通过 lua 脚本，延期之前会确保保证锁仍然属于自己
        _ = r.DelayExpire(ctx, WatchDogWorkStepSeconds)
    }
}
```



- **解锁时停止看门狗**

```go
// Unlock 解锁. 基于 lua 脚本实现操作原子性.
func (r *RedisLock) Unlock(ctx context.Context) (err error) {
    defer func() {
        if err != nil {
            return
        }


        // 停止看门狗
        if r.stopDog != nil {
            r.stopDog()
        }
    }()


    // ...
}
```



### 红锁（redlock）

<img src="https://typora-dusong.oss-cn-chengdu.aliyuncs.com/image-20250114%E4%B8%8B%E5%8D%8825758017.png" alt="image-20250114下午25758017" style="zoom:50%;" />

• 我们假定集群中有 2N+1个 redis 节点（通常将节点总数设置为奇数，有利于多数派原则的执行效率）

• 这些 redis 节点彼此间是相互独立的，不存在从属关系

• 每次客户端尝试进行加锁操作时，会同时对2N+1个节点发起加锁请求

• 每次客户端向一个节点发起加锁请求时，会设定一个很小的请求处理超时阈值

• 客户端依次对2N+1个节点发起加锁请求，只有在小于请求处理超时阈值的时间内完成了加锁操作，才视为一笔加锁成功的请求

• 过完2N+1个节点后，统计加锁成功的请求数量

• <u>倘若加锁请求成功数量大于等于N+1（多数派），则视为红锁加锁成功</u>

• <u>倘若加锁请求成功数量小于N+1，视为红锁加锁失败，此时会遍历2N+1个节点进行解锁操作</u>，有利于资源回收，提供后续使用方的取锁效率



> #### **实现方式**
>
> - **核心命令**：使用 `SET key value NX PX <ttl>`（原子性操作，设置键值 + 超时）。
> - **续期机制**：通过 Goroutine 定期续期（如使用 `PEXPIRE`），避免业务未完成锁超时。
> - **释放锁**：使用 Lua 脚本验证锁归属后删除（防止误删其他客户端的锁）。
> - **高可用方案**：Redis 集群 + Redlock 算法（多节点多数派加锁）。
>
> #### **优点**
>
> - **高性能**：Redis 内存操作，吞吐量高（适合高并发场景）。
> - **简单易用**：社区成熟库支持（如 `go-redis` 或 `redsync`）。
> - **灵活性**：支持锁超时自动释放，避免死锁。
>
> #### **缺点**
>
> - **弱一致性**：Redis 异步复制可能导致锁状态不一致（极端场景下可能失效）。
> - **脑裂风险**：主从切换时可能重复加锁（需配合 Redlock 缓解）。
>
> #### **适用场景**
>
> - **高频短任务**：如秒杀、库存扣减。
> - **容忍极低概率锁失效**：允许短暂不一致的业务。
>
> #### **Golang 示例（使用 `go-redis`）**
>
> ```go
> import (
>     "github.com/go-redis/redis/v8"
>     "github.com/go-redsync/redsync/v4"
>     "github.com/go-redsync/redsync/v4/redis/goredis/v8"
> )
> 
> func main() {
>     client := redis.NewClient(&redis.Options{Addr: "localhost:6379"})
>     pool := goredis.NewPool(client)
>     rs := redsync.New(pool)
> 
>     mutex := rs.NewMutex("my-lock")
>     if err := mutex.Lock(); err != nil {
>         panic(err)
>     }
>     defer mutex.Unlock()
>     // 执行业务逻辑
> }
> ```
>
> ------



## 2️⃣MySQL实现

- #### **实现方式**

  - **乐观锁**：通过版本号或 CAS（Compare-and-Swap）更新。
  - **悲观锁**：使用 `SELECT ... FOR UPDATE`（行级锁）。

  #### **缺点**

  - **性能差**：数据库锁竞争导致高延迟。
  - **死锁风险**：长事务容易引发死锁。
  - **不推荐**：仅适用于低频简单场景，或作为过渡方案。

## 3️⃣etcd实现

- **如何避免死锁问题**——租约机制

  <img src="https://typora-dusong.oss-cn-chengdu.aliyuncs.com/image-20250113%E4%B8%8B%E5%8D%8880333703.png" alt="image-20250113下午80333703" style="zoom:50%;" />

  1. 用户可以先申请一份租约，设定好租约的截止时间
  2. 异步启动一个续约协程，负责在业务逻辑处理完成前，按照一定的时间节奏持续进行续约操作
  3.  在执行取锁动作，将对应于锁的 kv 数据和租约进行关联绑定，使得锁数据和租约拥有相同的过期时间属性

- **惊群效应**

  倘若一把分布式锁的竞争比较激烈，那么锁的释放事件可能同时被多个的取锁方所监听，一旦锁真的被释放了，所有的取锁方都会一拥而上尝试取锁，对性能造成损耗

  - 解决方法：**前缀 + revision机制**：

    ![image-20250113下午83127287](https://typora-dusong.oss-cn-chengdu.aliyuncs.com/image-20250113%E4%B8%8B%E5%8D%8883127287.png)

    对于同一把分布式锁，取锁方获得一个相同前缀加上身份标识作为key，并获得该前缀下的一个递增版本号`revision`

    判定自身 lock key 对应的 revision 是不是其中最小的，如果是的话，才表示加锁成功

    如果锁被他人占用，取锁方会 watch 监听 revision 小于自己但最接近自己的那个 lock key 的删除事件.


> #### **实现方式**
>
> - **核心机制**：利用 Etcd 的 `Lease`（租约）和 `Txn`（事务）实现：
>   1. 创建一个租约（Lease），设置 TTL。
>   2. 通过事务（Txn）尝试写入一个唯一的键（如 `lock-key`），若不存在则写入成功，否则抢锁失败。
>   3. 绑定租约到键，自动续期保活。
>   4. 释放锁时删除键并撤销租约。
>
> #### **优点**
>
> - **强一致性**：基于 Raft 协议，保证锁状态全局一致。
> - **自动续期**：通过租约机制自动保活，避免业务未完成锁超时。
> - **可靠性高**：适合对锁强一致性要求高的场景。
>
> #### **缺点**
>
> - **性能较低**：相比 Redis，Etcd 的吞吐量较低（适合低频关键任务）。
> - **复杂度高**：需管理租约、事务等底层细节。
>
> #### **适用场景**
>
> - **关键业务**：如支付订单处理、分布式事务协调。
> - **强一致性要求**：不允许锁状态冲突或失效。
>
> #### **Golang 示例（使用 `etcd/clientv3`）**
>
> ```go
> import (
>     "context"
>     "go.etcd.io/etcd/client/v3"
>     "go.etcd.io/etcd/client/v3/concurrency"
> )
> 
> func main() {
>     cli, err := clientv3.New(clientv3.Config{Endpoints: []string{"localhost:2379"}})
>     if err != nil {
>         panic(err)
>     }
>     defer cli.Close()
> 
>     session, err := concurrency.NewSession(cli, concurrency.WithTTL(10)) // 设置租约的TTL为10秒
>     if err != nil {
>         panic(err)
>     }
>     defer session.Close()
> 
>     mutex := concurrency.NewMutex(session, "/my-lock")
>     if err := mutex.Lock(context.TODO()); err != nil {
>         panic(err)
>     }
>     defer mutex.Unlock(context.TODO())
>     // 执行业务逻辑
> }
> ```
