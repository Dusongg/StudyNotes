# 项目介绍

这是一个基于主从Reactor模型的高并发服务器，主reactor负责接受新连接，从reactor负责处理IO请求和业务逻辑，采用epoll实现IO多路复用，支持定时器管理连接超时，HTTP协议的解析与响应



🌟客户端从发送请求得到响应的过程

- 链接建立阶段

客户端发起连接请求——主Reactor(baseloop)通过epoll监听链接建立事件（epoll判断监听socket的全链接队列是否为空）——创建Connection对象（包含fd、buffer、http层设置的回调），并通过轮询的方式分配给一个从Reactor(subloop)处理

- 请求处理阶段：

从Reactor监听到可读事件，Connection调用handleRead接收数据——数据先存入Connection的输入缓冲区(*in_buffer)*——HttpContext解析HTTP请求（Connection收到数据后会直接调用message_callback来通知上层）

- 根据请求类型进行处理：

静态资源请求：直接读取文件内容

动态请求：调用对应的路由处理函数

- 启动写事件监控（调用channel对象的enableWrite，将事件标记为可写EPOLLOUT并更新到epoll当中，随后在下一轮epoll_wait后调用handleWrite写数据）

将响应数据写入输出缓冲区(*out_buffer)*——Connection的handleWrite将数据发送给客户端

如果是短连接则关闭连接，长连接则保持等待下一次请求

# 相关问题

## 什么是Reactor/Proactor

两者都是处理IO事件的并发模式

- Reactor

同步非阻塞IO， 通过IO多路复用等待事件就绪，Reactor检测到就绪事件，调用相应的回调函数通过read/write进行数据读写，处理完之后继续监听新事件到来

- Proactor

异步IO，程序提前提交IO请求，操作系统后台完成IO操作，Proactor收到完成通知，调用回调函数执行业务代码处理IO结果

## epoll中ET和LT的区别



## I/O多路复用

- **select：** 通过 **遍历+位图** 方式检查所有监听的文件描述符（fd）

  - 用户进程调用 select(fd_set, timeout)，将所有要监听的 fd 传给内核

  - 内核 **遍历** 这些 fd，并查询其状态

  - 用户进程获取返回的 fd，手动遍历 **找出可用的 fd** 进行处理。

    ```c 
    fd_set rfds;
    FD_ZERO(&rfds);
    FD_SET(sockfd, &rfds);
    
    struct timeval timeout = {5, 0};  // 超时时间 5 秒
    int ret = select(sockfd + 1, &rfds, NULL, NULL, &timeout);
    if (ret > 0 && FD_ISSET(sockfd, &rfds)) {
        // sockfd 可读，进行 read()
    }
    ```

- **poll**：poll 通过 **数组** 方式存储 fd，避免了 select 的 fd 限制，但仍然采用 **遍历** 方式检查 fd 状态，性能仍是 **O(n)**。

  ```c
  struct pollfd fds[1];
  fds[0].fd = sockfd;
  fds[0].events = POLLIN;  // 监听可读事件
  
  int ret = poll(fds, 1, 5000);  // 超时 5000ms
  if (ret > 0 && (fds[0].revents & POLLIN)) {
      // sockfd 可读，进行 read()
  }
  ```

- **epoll：**使用 **事件驱动（Event-Driven）** 的方式来管理大量文件描述符（fd）

  - 调用 epoll_create() 创建 epoll 实例，返回 epoll_fd；
  - 调用 epoll_ctl() **注册（EPOLL_CTL_ADD）** 需要监听的 fd 及其事件（如 EPOLLIN、EPOLLOUT）；
  - 调用 epoll_wait()，**阻塞等待事件发生**，返回 **发生事件的 fd**；

## io_uring是什么

io_uring 是 Linux 内核 **高性能异步 I/O（Async I/O）** 机制

✅ **基于环形缓冲区（Ring Buffer），用户态与内核共享**（减少 syscall 开销）

✅ **支持批量提交/获取 I/O 请求**（避免 syscall 逐个调用）

io_uring 由 **两个环形缓冲区（Ring Buffer）** 组成：

1️⃣ **提交队列（SQ, Submission Queue）**：用户态提交 I/O 请求

2️⃣ **完成队列（CQ, Completion Queue）**：内核完成 I/O 后通知用户态



```c
#include <liburing.h>
#include <fcntl.h>
#include <stdio.h>
#include <string.h>
#include <unistd.h>

#define BUFFER_SIZE 4096

int main() {
    struct io_uring ring;
    io_uring_queue_init(8, &ring, 0);  // 初始化 io_uring，队列大小 8

    int fd = open("test.txt", O_RDONLY);
    if (fd < 0) {
        perror("open failed");
        return 1;
    }

    struct io_uring_sqe *sqe = io_uring_get_sqe(&ring);  // 获取提交队列元素（SQE）
    char buffer[BUFFER_SIZE] = {0};
    struct iovec iov = { .iov_base = buffer, .iov_len = BUFFER_SIZE };
    
    io_uring_prep_readv(sqe, fd, &iov, 1, 0);  // 准备异步读请求
    io_uring_submit(&ring);  // 提交请求到内核
    
    struct io_uring_cqe *cqe;
    io_uring_wait_cqe(&ring, &cqe);  // 等待完成队列（CQE）
    printf("Read %d bytes: %s\n", cqe->res, buffer);

    io_uring_cqe_seen(&ring, cqe);  // 标记 CQE 已处理
    io_uring_queue_exit(&ring);
    close(fd);
    return 0;
}
```



## 压力测试工具

使用wrk

```bash
# 使用12个线程运行30秒，保持400个HTTP连接
wrk -t12 -c400 -d30s http://localhost:8080/

# 重要参数说明：
# -t: 线程数
# -c: 并发连接数
# -d: 测试时间
# -s: 指定Lua脚本
```

### [性能指标](https://mikechen.cc/15729.html)

- QPS

- TPS：

  > QPS和TPS的区别：
  >
  > **一个事务可能包含多个查询**

- RT响应时间

- 并发数





## 影响服务器链接数的因素

**1️⃣ 文件描述符数量限制**

- 段错误 ---->   文件打开数量限制 `ulimit -a`

![image-20240306231537525](https://typora-dusong.oss-cn-chengdu.aliyuncs.com/image-20240306231537525.png)

> 1. 修改ulimit -n 1048576`（2^20）
>
> 2. 同时修改程序链接数组的大小，不采用固定大小
>
> 3. 另外，`fs.file-max = 1048576 `
>
>    ```bash
>    sudo vim /etc/sysctl.conf
>    #如果没设置则添加
>    #设置完后刷新生效
>    sudo sysctl -p
>       
>    ```



**2️⃣ 端口数量限制**

- 一个`socket` ：`fd`  + `tcb(sip, dip, sport, dport, proto)`， 对于客户端来说，源端口（两个字节），最大65535，并且0-1023的端口为系统特定端口不能随机分配，所以当链接数量达到 一定数量(65535-1024)时，链接无法再建立了

> 解决办法：考虑链接的五元组
>
> 1. 多用几个客户端，保证源ip不一样-> n * (65535-1024)
> 2. 多用几个目的端口 -> n * (65535-1024)，同时客户端连接时也用链接到服务端的多个端口

![image-20240309234111486](https://typora-dusong.oss-cn-chengdu.aliyuncs.com/image-20240309234111486.png)

- 端口范围限制：

> 将 
>
> ```bash
> vim /etc/sysctl.conf
> ```
>
> 中的`net.ipv4.ip_local_port_range = 1-24 65535`拷贝到
>
> ```bash
> sudo vim /etc/sysctl.conf
> 
> #拷贝完后生效
> sudo sysctl -p
> ```



**3️⃣ 内核缓冲区限制**

- 改服务端的tcb配置（链接双方下面几个配置保持一样）

![image-20240307001227946](https://typora-dusong.oss-cn-chengdu.aliyuncs.com/image-20240307001227946.png)

```bash
sudo vim /etc/sysctl.conf
#写入上面几个配置
sudo sysctl -p
```

执行完上面命令之后，报错：

![image-20240307001544910](https://typora-dusong.oss-cn-chengdu.aliyuncs.com/image-20240307001544910.png)

执行：`sudo modprobe ip_conntrack`

- 增加`/etc/sysctl.conf`中的`net.ipv4.tcp_mem ` 、 `net.ipv4.tcp_wmem` 、 `net.ipv4.tcp_rmem`



